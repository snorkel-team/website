---
layout: homepage
title: Features and Capabilities
---

<div class="hero-subheader">
  <div class="container">
    <div class="row row-spacing mobile-padding">
      <div class="col-sm-12">
        <h1>{{ page.title }}</h1>
      </div>
    </div>
    <div class="row row-spacing vertical-align mobile-padding">
      <div class="col-sm-6">
        <h3>Programming Training Data</h3>
        <p>Snorkel is a system for programmatically building and managing training datasets to rapidly and flexibly fuel machine learning models. Snorkel provides a radically faster and more flexible to build machine learning applications, by letting users programmatically build and manipulate training data rather than label it by hand. Snorkel focuses on three key operations: labeling data, for example using heuristic rules or distant supervision techniques; transforming data, for example to perform data augmentation and express invariances in the data; and slicing data into different critical subsets.
        </p>
      </div>
      <div class="col-sm-6">
        <img src="/doks-theme/assets/images/layout/TrainingData.png" alt="Training Data Operations">
      </div>
    </div>
    <hr>
    <br>
    <div class="row row-spacing mobile-padding">
      <div class="col-sm-5">
        <p class="purple-numbers">01</p>
        <h4>Labeling</h4>
        <p>
          Snorkel lets users write labeling functions (LFs) to heuristically or noisily label some subset of the training examples. Snorkel then models the quality and correlations of these LFs using novel, theoretically-grounded statistical modeling techniques.
        </p>
        <button href="http://hazyresearch.github.io/snorkel/blog/weak_supervision.html" class="btn btn--rounded btn--dark">Blog</button>
        <button href="snorkel.org" class="btn btn--rounded btn--dark">Tutorial</button>
        <button href="https://arxiv.org/abs/1711.10160" class="btn btn--rounded btn--dark">VLDB Paper</button>
        <!-- <button href="https://arxiv.org/abs/1605.07723" class="btn btn--rounded btn--dark">NeurIPS Paper</button>
        <button href="https://arxiv.org/abs/1810.02840" class="btn btn--rounded btn--dark">AAAI Paper</button> -->
      </div>
      <div class="col-sm-1"></div>
      <div class="col-sm-6">
        <img src="/doks-theme/assets/images/layout/Labeling.png" alt="Labeling" style="margin-top: 110px;" />
      </div>
    </div>
    <div class="row row-spacing mobile-padding">
      <div class="col-sm-5 hidden-xs">
        <img src="/doks-theme/assets/images/layout/Transforming.png" alt="Transforming" style="margin-top: 110px;" />
      </div>
      <div class="col-sm-1"></div>
      <div class="col-sm-6">
        <p class="purple-numbers">02</p>
        <h4>Transforming</h4>
        <p>
          Snorkel lets users write transformation functions (TFs) to heuristically generate new, modified training examples by transforming existing ones---a strategy often referred to as data augmentation. Rather than requiring users to tune these data augmentation or transformation strategies by hand, Snorkel learns compositions of transformations across various domain-specific tasks to optimize for a representative training set. Links to blogs and papers.
        </p>
        <button href="https://hazyresearch.github.io/snorkel/blog/tanda.html" class="btn btn--rounded btn--dark">Blog</button>
        <button href="snorkel.org" class="btn btn--rounded btn--dark">Tutorial</button>
        <button href="https://arxiv.org/abs/1803.06084" class="btn btn--rounded btn--dark">NeurIPS Paper</button>
        <!-- <button href="https://arxiv.org/abs/1803.06084" class="btn btn--rounded btn--dark">ICML Paper</button> -->
      </div>
      <div class="col-sm-12 visible-xs-block">
        <img src="/doks-theme/assets/images/layout/Transforming.png" alt="Transforming" style="margin-top: 110px;" />
      </div>
    </div>
    <div class="row row-spacing mobile-padding">
      <div class="col-sm-5">
        <p class="purple-numbers">03</p>
        <h4>Slicing</h4>
        <p>
          Snorkel lets users write slicing functions (SFs) to heuristically identify subsets of the data the model should particularly care about, e.g. have extra representative capacity for, due to their difficulty and/or importance. It models slices in the style of multi-task learning and an attention-mechanism is then learned over these heads.  Links to blogs and papers.
        </p>
        <button href="http://hazyresearch.github.io/snorkel/blog/superglue.html" class="btn btn--rounded btn--dark">Blog</button>
        <button href="snorkel.org" class="btn btn--rounded btn--dark">Tutorial</button>
      </div>
      <div class="col-sm-1"></div>
      <div class="col-sm-6">
        <img src="/doks-theme/assets/images/layout/Slicing.png" alt="Slicing" style="margin-top: 80px;" />
      </div>
    </div>
  </div>
</div>
